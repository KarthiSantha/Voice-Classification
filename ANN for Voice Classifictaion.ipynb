{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for vizualization\n",
    "from matplotlib.pyplot import figure # for figuresize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_data = pd.read_csv(\"/home/karthi_krish/Voice-Classification/voice.csv\")\n",
    "voice_data.label = [1 if each == \"male\" else 0 for each in voice_data.label]\n",
    "voice_data.head() # check if 1-0 conversion worked\n",
    "\n",
    "\n",
    "#normalization\n",
    "Y_data = voice_data.label.values\n",
    "X_data = voice_data.drop([\"label\"], axis = 1)\n",
    "X_data = (X_data - X_data.min())/(X_data.max() - X_data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3168, 21)\n",
      "(3168, 20)\n",
      "(3168,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(voice_data.shape)\n",
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.3, random_state = 42)\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n",
    "    \n",
    "    parameters = {\"W1\": np.random.randn(2,x_train.shape[0]) * 0.1,\n",
    "                  \"b1\": np.zeros((2,1)),\n",
    "                  \"W2\": np.random.randn(2,2) * 0.1,\n",
    "                  \"b2\": np.zeros((2,1)),\n",
    "                  \"W3\": np.random.randn(1,2) * 0.1,\n",
    "                  \"b3\": np.zeros((1,1))}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 20)\n",
      "(2, 2)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(parameters[\"W1\"].shape)\n",
    "print(parameters[\"W2\"].shape)\n",
    "print(parameters[\"W3\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigmoid Function\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation_NN(x_train, parameters):\n",
    "    \n",
    "    Z1 = np.dot(parameters[\"W1\"],x_train) + parameters[\"b1\"]\n",
    "    A1 = np.tanh(Z1) # tanh is used as activation function 1\n",
    "    Z2 = np.dot(parameters[\"W2\"],A1) + parameters[\"b2\"]\n",
    "    A2 = np.tanh(Z2) # tanh is used as activation function 2\n",
    "    Z3 = np.dot(parameters[\"W3\"],A2) + parameters[\"b3\"]\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"W1\": parameters[\"W1\"],\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2,\n",
    "             \"W2\": parameters[\"W2\"],\n",
    "             \"Z3\": Z3,\n",
    "             \"A3\": A3,\n",
    "             \"W3\": parameters[\"W3\"]}\n",
    "    \n",
    "    return A3, cache\n",
    "\n",
    "A3, cache = forward_propagation_NN(x_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_NN(A3, Y, parameters):\n",
    "    \n",
    "    logprobs = np.multiply(np.log(A3),Y)\n",
    "    cost = -np.sum(logprobs)/Y.shape[1]\n",
    "    \n",
    "    return cost\n",
    "\n",
    "cost = compute_cost_NN(A3, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_propagation_NN(parameters, cache, X, Y):\n",
    "    \n",
    "    dimension = X.shape[0] # it is 20 for our case\n",
    "    dZ3 = cache[\"A3\"] - Y # d(cost)/d(Z3)\n",
    "    dW3 = 1/dimension * np.dot(dZ3,cache[\"A2\"].T) # d(cost)/d(W3)\n",
    "    db3 = 1/dimension * np.sum(dZ3, axis=1, keepdims=True) # d(cost)/d(b3)\n",
    "    dZ2 = np.multiply(np.dot(dZ3.T, cache[\"W3\"]).T , 1-np.power(cache[\"A2\"],2)) # d(cost)/d(Z2)\n",
    "    dW2 = 1/dimension * np.dot(cache[\"A1\"], dZ2.T) # d(cost)/d(W2)\n",
    "    db2 = 1/dimension * np.sum(dZ2, axis=1, keepdims=True) # d(cost)/d(b2)\n",
    "    dZ1 = np.multiply(np.dot(dZ2.T, cache[\"W2\"].T).T,1-np.power(cache[\"A1\"],2)) # d(cost)/d(Z1)\n",
    "    dW1 = 1/dimension * np.dot(dZ1, X.T) # d(cost)/d(W1)\n",
    "    db1 = 1/dimension * np.sum(dZ1,axis=1, keepdims=True) # d(cost)/d(b1)\n",
    "    grads = {'dW3':dW3, \n",
    "             'db3':db3,\n",
    "             'dW2':dW2,\n",
    "             'db2':db2,\n",
    "             'dW1':dW1,\n",
    "             'db1':db1}\n",
    "    \n",
    "    return grads\n",
    "\n",
    "grads = backward_propagation_NN(parameters, cache, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_NN(parameters, grads, learning_rate = Learning_Rate):\n",
    "    parameters = {\"W1\": parameters[\"W1\"]-learning_rate*grads[\"dW1\"],\n",
    "                  \"b1\": parameters[\"b1\"]-learning_rate*grads[\"db1\"],\n",
    "                  \"W2\": parameters[\"W2\"]-learning_rate*grads[\"dW2\"],\n",
    "                  \"b2\": parameters[\"b2\"]-learning_rate*grads[\"db2\"],\n",
    "                  \"W3\": parameters[\"W3\"]-learning_rate*grads[\"dW3\"],\n",
    "                  \"b3\": parameters[\"b3\"]-learning_rate*grads[\"db3\"]}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "parameters = update_parameters_NN(parameters, grads, learning_rate = Learning_Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_NN(parameters,x_test):\n",
    "    # x_test is the input for forward propagation\n",
    "    A3, cache = forward_propagation_NN(x_test,parameters)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "\n",
    "    for i in range(A3.shape[1]):\n",
    "        if A3[0,i]<= 0.5: # if smaller than 0.5, predict it as 0\n",
    "            Y_prediction[0,i] = 0\n",
    "        else: # if greater than 0.5, predict it as 1\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction\n",
    "\n",
    "Y_prediction = predict_NN(parameters,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
